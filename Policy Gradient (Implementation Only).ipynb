{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7cb945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-cpu\n",
    "#pip install gymnasium\n",
    "#pip install stable-baselines3\n",
    "#pip install tensorflow-probability\n",
    "\n",
    "#for project\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gymnasium import Wrapper\n",
    "import time\n",
    "\n",
    "#stable-baselines3 (what isaac used, for consistency)\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "#from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "#for gradient policy\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5655cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS ONLY IF YOU WANT TO USE COMBINED REWARDS\n",
    "#the only change we made is to +100 for leg touching the ground (instead of the default +10)\n",
    "class CustomRewardWrapper(Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(CustomRewardWrapper, self).__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, original_reward, truncated, terminated, info = self.env.step(action)\n",
    "        reward = self.custom_reward(state, terminated or truncated, action, original_reward)\n",
    "        return state, reward, truncated, terminated, info\n",
    "\n",
    "    def custom_reward(self, state, is_terminal, action, original_reward):\n",
    "        (_, _, _, _, _, _, leg_1_contact, leg_2_contact) = state\n",
    "\n",
    "        reward = original_reward # maintain original rewards\n",
    "        # Reward for leg contact\n",
    "        if leg_1_contact:\n",
    "            reward += 90 # default was +10\n",
    "        if leg_2_contact:\n",
    "            reward += 90 # default was +10\n",
    "\n",
    "        return reward\n",
    "\n",
    "def make_env():\n",
    "    env = Monitor(gym.make(\"LunarLander-v2\", render_mode=\"human\"))\n",
    "    env = CustomRewardWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d00be7",
   "metadata": {},
   "source": [
    "## Task 1: Install gym, and play Lunar Lander with random control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#render_mode='human' lets us visualise the lander crashing :)\n",
    "def make_env():\n",
    "    return gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "env = make_env()\n",
    "#lets watch the lander crash 10 times :)\n",
    "episodes = 10\n",
    "for episode in range(episodes):\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        step+=1\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state,reward,done,info,_ = env.step(action)\n",
    "        score+=reward\n",
    "        #print('Episode:{} Step:{} Score:{}'.format(episode+1, step, score))  # Print the episode, step, and current score\n",
    "    print('Episode:{} score:{}'.format(episode+1,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f7945",
   "metadata": {},
   "source": [
    "## Task 2: Implementing Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy Gradient seeks to approximate agent's policy (probability distribution over the action space given state)\n",
    "\n",
    "#Policy Gradient Neural Network\n",
    "class PolicyGradient(keras.Model):\n",
    "    def __init__(self, num_actions, fc1num=256, fc2num=256):\n",
    "        super(PolicyGradient,self).__init__()\n",
    "        #Number of neurons for each fully connected layer\n",
    "        self.fc1num = fc1num\n",
    "        self.fc2num = fc2num\n",
    "        self.num_actions = num_actions\n",
    "        #Creating the fc1, fc2, fco (output) layers\n",
    "        self.fc1 = Dense(self.fc1num, activation='relu')\n",
    "        self.fc2 = Dense(self.fc2num, activation='relu')\n",
    "        self.fco = Dense(self.num_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, state):\n",
    "        #Feedforward, outputs policy pi (probabilities)\n",
    "        value = self.fc1(state)\n",
    "        value = self.fc2(value)\n",
    "        pi = self.fco(value)\n",
    "        return pi\n",
    "    \n",
    "#Agent functions\n",
    "class Agent:\n",
    "    def __init__(self, lrate=0.001, gamma=0.99, num_actions=4, fc1num=256, fc2num=256):\n",
    "        #lrate is the learning rate\n",
    "        self.lr=lrate\n",
    "        #gamma is the discount factor (from 0 to 1)\n",
    "        self.gamma=gamma\n",
    "        #4 default actions (nothing, left, down, right)\n",
    "        self.num_actions=num_actions\n",
    "        #3 memory arrays (cleared after every episode)\n",
    "        self.state_memory=[]\n",
    "        self.action_memory=[]\n",
    "        self.reward_memory=[]\n",
    "        #the policy gradient network\n",
    "        self.policy=PolicyGradient(num_actions=num_actions, fc1num=fc1num, fc2num=fc2num)\n",
    "        self.policy.compile(optimizer=Adam(learning_rate=self.lr))\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        state=tf.convert_to_tensor([observation],dtype=tf.float32)\n",
    "        #sample random action based on the probability distribution\n",
    "        probs=self.policy(state)\n",
    "        action_probs=tfp.distributions.Categorical(probs=probs)\n",
    "        action=action_probs.sample()\n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    def store_transition(self, observation,action,reward):\n",
    "        #store transition info to memory arrays\n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        #convert the actions and rewards to tensorflow tensor\n",
    "        actions=tf.convert_to_tensor(self.action_memory,dtype=tf.float32)\n",
    "        rewards=np.array(self.reward_memory)\n",
    "        #calculate G, the discounted sum of future rewards at each time step\n",
    "        G = np.zeros_like(rewards)\n",
    "        for t in range(len(rewards)):\n",
    "            G_sum=0\n",
    "            discount=1\n",
    "            for k in range(t,len(rewards)):\n",
    "                G_sum+=rewards[k]*discount\n",
    "                discount*=self.gamma\n",
    "            G[t]=G_sum\n",
    "        \n",
    "        #normalizing rewards to promote stability (for nn)                                       \n",
    "        #std = np.std(G) if np.std(G)>0 else 1\n",
    "        #G = (G-np.mean(G))/std\n",
    "        \n",
    "        #calculate gradients wrt params of deep nn\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss=0\n",
    "            for idx,(g,state) in enumerate(zip(G,self.state_memory)):\n",
    "                state=tf.convert_to_tensor([state],dtype=tf.float32)\n",
    "                probs=self.policy(state)\n",
    "                #clipping because we are taking the log (avoid log0)                              \n",
    "                probs = tf.clip_by_value(probs, clip_value_min=1e-8, clip_value_max=1-1e-8)\n",
    "                action_probs=tfp.distributions.Categorical(probs=probs)\n",
    "                log_prob=action_probs.log_prob(actions[idx])\n",
    "                loss += (-g)*tf.squeeze(log_prob)\n",
    "                \n",
    "        gradient=tape.gradient(loss,self.policy.trainable_variables)\n",
    "        self.policy.optimizer.apply_gradients(zip(gradient,self.policy.trainable_variables))\n",
    "        \n",
    "        #reset memory (monte carlo method)\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb22ee",
   "metadata": {},
   "source": [
    "### Default run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "lr0 = 0.0003\n",
    "gamma0 = 0.99\n",
    "episodes = 5000\n",
    "fcdim = 256\n",
    "time_threshold = 40\n",
    "\n",
    "agent=Agent(lrate=lr0,gamma=gamma0,num_actions=4, fc1num=fcdim, fc2num=fcdim)\n",
    "env=gym.make(\"LunarLander-v2\")\n",
    "scores=[]\n",
    "avgscores=[]\n",
    "for episode in range(episodes):\n",
    "    start_time = time.time()\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        n_state,reward,done,info,_ = env.step(action)\n",
    "        if time.time()-start_time>=time_threshold:\n",
    "            done=True\n",
    "            reward-=50\n",
    "        agent.store_transition(observation=state,action=action,reward=reward)\n",
    "        state=n_state\n",
    "        score+=reward\n",
    "    scores.append(score)\n",
    "    agent.learn()\n",
    "    ave_score = np.mean(scores[-50:])\n",
    "    avgscores.append(ave_score)\n",
    "    print('Episode:{} score:{} avgscore:{}'.format(episode+1,score,ave_score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot individual scores\n",
    "plt.plot(range(len(scores)), scores, label='Scores', alpha=0.6)\n",
    "\n",
    "#plot smooth scores (averaged over every 50 episodes)\n",
    "#use red to distinguish it from training\n",
    "plt.plot(range(len(avgscores)), avgscores, color='red', label='Averaged Scores', alpha=0.6)\n",
    "\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Scores per Episode (Default Rewards, Default Hyperparams)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#finding the highest reward\n",
    "print(\"Highest score is at %d episodes with a score of %.5f.\" %(scores.index(max(scores))+1, max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a634f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "agent.policy.save(\"policygradient_1.h5\")\n",
    "\n",
    "#Load a model\n",
    "#model = keras.models.load_model(\"policygradient_1.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
